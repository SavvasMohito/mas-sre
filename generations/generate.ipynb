{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Individual Crews\n",
        "\n",
        "This notebook allows you to test and debug each crew independently.\n",
        "\n",
        "**Setup:**\n",
        "1. Make sure your `.env` file has OPENAI_API_KEY\n",
        "2. Run cells in order (or jump to specific crew)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "PARTICIPANT_NAME = \"adam\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ OpenAI API key loaded\n",
            "âœ“ Using model: gpt-4o-mini\n",
            "âœ“ Setup complete\n"
          ]
        }
      ],
      "source": [
        "# Setup and imports\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, str(Path.cwd() / \"src\"))\n",
        "\n",
        "# Load environment\n",
        "load_dotenv()\n",
        "\n",
        "# Set OpenAI model for CrewAI\n",
        "os.environ[\"OPENAI_MODEL_NAME\"] = \"gpt-4o-mini\"  # or gpt-4o, gpt-4-turbo\n",
        "# Verify API key is loaded\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    print(\"âš ï¸  WARNING: OPENAI_API_KEY not found in environment!\")\n",
        "    print(\"   Please add it to your .env file\")\n",
        "else:\n",
        "    print(\"âœ“ OpenAI API key loaded\")\n",
        "    print(f\"âœ“ Using model: {os.getenv('OPENAI_MODEL_NAME')}\")\n",
        "\n",
        "print(\"âœ“ Setup complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking Weaviate connection...\n",
            "âœ“ Weaviate is connected and ready\n",
            "âœ“ SecurityControl collection exists\n"
          ]
        }
      ],
      "source": [
        "# Weaviate Configuration and Health Check\n",
        "import weaviate\n",
        "\n",
        "# Set Weaviate connection parameters\n",
        "os.environ.setdefault(\"WEAVIATE_HOST\", \"localhost\")\n",
        "os.environ.setdefault(\"WEAVIATE_PORT\", \"8080\")\n",
        "os.environ.setdefault(\"WEAVIATE_GRPC_PORT\", \"50051\")\n",
        "\n",
        "print(\"Checking Weaviate connection...\")\n",
        "try:\n",
        "    client = weaviate.connect_to_local(\n",
        "        host=os.getenv(\"WEAVIATE_HOST\"),\n",
        "        port=int(os.getenv(\"WEAVIATE_PORT\")),\n",
        "        grpc_port=int(os.getenv(\"WEAVIATE_GRPC_PORT\")),\n",
        "    )\n",
        "\n",
        "    # Check if connected\n",
        "    if client.is_ready():\n",
        "        print(\"âœ“ Weaviate is connected and ready\")\n",
        "\n",
        "        # Check if SecurityControl collection exists\n",
        "        if client.collections.exists(\"SecurityControl\"):\n",
        "            collection = client.collections.get(\"SecurityControl\")\n",
        "            # Try to count objects (may not work in all versions)\n",
        "            print(\"âœ“ SecurityControl collection exists\")\n",
        "        else:\n",
        "            print(\"âš ï¸  WARNING: SecurityControl collection does not exist!\")\n",
        "            print(\"   Run: python -m security_requirements_system.tools.weaviate_setup\")\n",
        "    else:\n",
        "        print(\"âš ï¸  WARNING: Weaviate is not ready\")\n",
        "\n",
        "    client.close()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ERROR: Cannot connect to Weaviate: {e}\")\n",
        "    print(f\"   Make sure Weaviate is running on {os.getenv('WEAVIATE_HOST')}:{os.getenv('WEAVIATE_PORT')}\")\n",
        "    print(\"   Start with: docker-compose up -d\")\n",
        "    print(\"\\nWeaviate Configuration:\")\n",
        "    print(f\"  Host: {os.getenv('WEAVIATE_HOST')}\")\n",
        "    print(f\"  Port: {os.getenv('WEAVIATE_PORT')}\")\n",
        "    print(f\"  gRPC Port: {os.getenv('WEAVIATE_GRPC_PORT')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Helper functions loaded\n",
            "  Project root: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations\n",
            "  adam outputs dir: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations/adam/outputs\n",
            "  adam outputs exists: True\n"
          ]
        }
      ],
      "source": [
        "# Helper functions\n",
        "\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "OUTPUTS_DIR = PROJECT_ROOT / PARTICIPANT_NAME / \"outputs\"\n",
        "\n",
        "if not OUTPUTS_DIR.exists():\n",
        "    OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def display_crew_output(result, crew_name):\n",
        "    \"\"\"Display crew output in a readable format.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{crew_name} OUTPUT\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    if hasattr(result, \"raw\"):\n",
        "        print(\"RAW OUTPUT:\")\n",
        "        print(result.raw[:500] + \"...\" if len(result.raw) > 500 else result.raw)\n",
        "\n",
        "    if hasattr(result, \"pydantic\") and result.pydantic:\n",
        "        print(\"\\nSTRUCTURED OUTPUT:\")\n",
        "        print(json.dumps(result.pydantic.model_dump(), indent=2, default=str)[:1000])\n",
        "\n",
        "    if hasattr(result, \"tasks_output\"):\n",
        "        print(f\"\\nTASKS: {len(result.tasks_output)}\")\n",
        "        for i, task in enumerate(result.tasks_output):\n",
        "            task_name = task.name if hasattr(task, \"name\") else f\"Task {i}\"\n",
        "            print(f\"  {i+1}. {task_name}\")\n",
        "            if hasattr(task, \"pydantic\") and task.pydantic:\n",
        "                print(f\"     Type: {type(task.pydantic).__name__}\")\n",
        "\n",
        "\n",
        "def save_output(result, crew_name, output_dir=None):\n",
        "    \"\"\"Save crew output to JSON file.\"\"\"\n",
        "    if output_dir is None:\n",
        "        output_path = OUTPUTS_DIR\n",
        "    else:\n",
        "        output_path = Path(output_dir)\n",
        "\n",
        "    output_path.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    output_file = output_path / f\"{crew_name}_output.json\"\n",
        "\n",
        "    data = {}\n",
        "    if hasattr(result, \"raw\"):\n",
        "        data[\"raw\"] = result.raw\n",
        "    if hasattr(result, \"pydantic\") and result.pydantic:\n",
        "        data[\"pydantic\"] = result.pydantic.model_dump()\n",
        "    if hasattr(result, \"tasks_output\"):\n",
        "        data[\"tasks\"] = []\n",
        "        for task in result.tasks_output:\n",
        "            task_data = {\"name\": task.name if hasattr(task, \"name\") else \"unknown\", \"raw\": task.raw if hasattr(task, \"raw\") else str(task)}\n",
        "            if hasattr(task, \"pydantic\") and task.pydantic:\n",
        "                task_data[\"pydantic\"] = task.pydantic.model_dump()\n",
        "            data[\"tasks\"].append(task_data)\n",
        "\n",
        "    with open(output_file, \"w\") as f:\n",
        "        json.dump(data, f, indent=2, default=str)\n",
        "\n",
        "    print(f\"\\nðŸ’¾ Saved to: {output_file}\")\n",
        "    return output_file\n",
        "\n",
        "\n",
        "def load_cached_output(crew_name, output_dir=None):\n",
        "    \"\"\"Load cached crew output from JSON file if it exists.\"\"\"\n",
        "    if output_dir is None:\n",
        "        output_path = OUTPUTS_DIR\n",
        "    else:\n",
        "        output_path = Path(output_dir)\n",
        "\n",
        "    output_file = output_path / f\"{crew_name}_output.json\"\n",
        "\n",
        "    if output_file.exists():\n",
        "        with open(output_file, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "        print(f\"ðŸ“‚ Loaded cached output from: {output_file}\")\n",
        "        return data\n",
        "    else:\n",
        "        print(f\"âš ï¸  No cached output found for {crew_name}\")\n",
        "        print(f\"   Looked in: {output_file}\")\n",
        "        print(f\"   Project root: {PROJECT_ROOT}\")\n",
        "        print(f\"   {PARTICIPANT_NAME} outputs dir: {OUTPUTS_DIR}\")\n",
        "        print(f\"   {PARTICIPANT_NAME} outputs exists: {OUTPUTS_DIR.exists()}\")\n",
        "        if OUTPUTS_DIR.exists():\n",
        "            files = list(OUTPUTS_DIR.glob(\"*.json\"))\n",
        "            print(f\"   Files in test_outputs: {len(files)} files\")\n",
        "            if files:\n",
        "                print(f\"   Sample files: {[f.name for f in files[:5]]}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "print(\"âœ“ Helper functions loaded\")\n",
        "print(f\"  Project root: {PROJECT_ROOT}\")\n",
        "print(f\"  {PARTICIPANT_NAME} outputs dir: {OUTPUTS_DIR}\")\n",
        "print(f\"  {PARTICIPANT_NAME} outputs exists: {OUTPUTS_DIR.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Cached Outputs\n",
        "\n",
        "Each crew cell below will automatically load required data from cached JSON files if the variables don't exist in memory. This allows you to:\n",
        "- Jump to any crew without running previous cells\n",
        "- Reuse outputs from previous runs\n",
        "- Speed up testing\n",
        "\n",
        "If a required cache file is missing, you'll see an error prompting you to run the prerequisite crew first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Sample requirements loaded:\n",
            "\n",
            "Omnichannel Prolog Agentic Blogging System Requirements\n",
            "\n",
            "We need to build a web application for collaborative, agentic blogging across multiple channels.\n",
            "\n",
            "Key Features:\n",
            "\n",
            "1. User Management\n",
            "   - User registration, login, and profiles\n",
            "   - Agent and human role assignments (Admin, Editor, Contributor, Agent)\n",
            "   - Channel workspace creation and management\n",
            "\n",
            "2. Content Management\n",
            "  - Create, edit, and delete blog posts\n",
            "  - Assign posts to agents or users\n",
            "  - Schedule and tag posts for multichannel publication\n",
            "  - Add comments and attachments to posts\n",
            "  - Track post status (Draft, In Review, Published)\n",
            "\n",
            "3. Collaboration\n",
            "  - Real-time post updates and status changes\n",
            "  - Threaded comments on posts\n",
            "  - @mention notifications for contributors and agents\n",
            "  - Activity feed of recent system actions\n",
            "\n",
            "4. Agent Integration\n",
            "  - Prolog-driven continuous agent publishing and moderation\n",
            "  - Automated topic suggestion and content enrichment\n",
            "  - Rule-based workflow coordination between agents and users\n",
            "\n",
            "5. Channel Management\n",
            "  - Connect and manage multiple publishing channels (Web, Email, Social)\n",
            "  - Unified dashboard for content performance per channel\n",
            "  - Channel-specific formatting and scheduling\n",
            "\n",
            "6. Notifications\n",
            "  - Email and in-app notifications for mentions, assignments, and publishing events\n",
            "  - Daily/weekly digest of channel/blog activity\n",
            "\n",
            "7. Reporting\n",
            "  - Dashboard with content and channel analytics\n",
            "  - Agent and contributor productivity metrics\n",
            "  - Export blog lists, agent actions, and analytics\n",
            "\n",
            "The app will be web-based, storing user data, posts, files, agent logs, and supporting integrations with email, social media platforms, and third-party communication services.\n"
          ]
        }
      ],
      "source": [
        "# Read requirements text\n",
        "\n",
        "with open(f\"{PARTICIPANT_NAME}/{PARTICIPANT_NAME}.md\", \"r\") as f:\n",
        "    INPUT_REQUIREMENTS = f.read()\n",
        "\n",
        "print(\"âœ“ Sample requirements loaded:\\n\")\n",
        "print(INPUT_REQUIREMENTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš ï¸  No cached output found for requirements_analysis\n",
            "   Looked in: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations/adam/outputs/requirements_analysis_output.json\n",
            "   Project root: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations\n",
            "   adam outputs dir: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations/adam/outputs\n",
            "   adam outputs exists: True\n",
            "   Files in test_outputs: 0 files\n",
            "âš ï¸  No cached output found for stakeholder\n",
            "   Looked in: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations/adam/outputs/stakeholder_output.json\n",
            "   Project root: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations\n",
            "   adam outputs dir: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations/adam/outputs\n",
            "   adam outputs exists: True\n",
            "   Files in test_outputs: 0 files\n",
            "âš ï¸  No cached output found for threat_modeling\n",
            "   Looked in: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations/adam/outputs/threat_modeling_output.json\n",
            "   Project root: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations\n",
            "   adam outputs dir: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations/adam/outputs\n",
            "   adam outputs exists: True\n",
            "   Files in test_outputs: 0 files\n",
            "âš ï¸  No cached output found for domain_security\n",
            "   Looked in: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations/adam/outputs/domain_security_output.json\n",
            "   Project root: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations\n",
            "   adam outputs dir: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations/adam/outputs\n",
            "   adam outputs exists: True\n",
            "   Files in test_outputs: 0 files\n",
            "âš ï¸  No cached output found for llm_security\n",
            "   Looked in: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations/adam/outputs/llm_security_output.json\n",
            "   Project root: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations\n",
            "   adam outputs dir: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations/adam/outputs\n",
            "   adam outputs exists: True\n",
            "   Files in test_outputs: 0 files\n",
            "âš ï¸  No cached output found for compliance\n",
            "   Looked in: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations/adam/outputs/compliance_output.json\n",
            "   Project root: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations\n",
            "   adam outputs dir: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations/adam/outputs\n",
            "   adam outputs exists: True\n",
            "   Files in test_outputs: 0 files\n",
            "âš ï¸  No cached output found for security_architecture\n",
            "   Looked in: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations/adam/outputs/security_architecture_output.json\n",
            "   Project root: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations\n",
            "   adam outputs dir: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations/adam/outputs\n",
            "   adam outputs exists: True\n",
            "   Files in test_outputs: 0 files\n",
            "âš ï¸  No cached output found for roadmap\n",
            "   Looked in: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations/adam/outputs/roadmap_output.json\n",
            "   Project root: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations\n",
            "   adam outputs dir: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations/adam/outputs\n",
            "   adam outputs exists: True\n",
            "   Files in test_outputs: 0 files\n",
            "âš ï¸  No cached output found for verification\n",
            "   Looked in: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations/adam/outputs/verification_output.json\n",
            "   Project root: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations\n",
            "   adam outputs dir: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations/adam/outputs\n",
            "   adam outputs exists: True\n",
            "   Files in test_outputs: 0 files\n",
            "âš ï¸  No cached output found for validation\n",
            "   Looked in: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations/adam/outputs/validation_output.json\n",
            "   Project root: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations\n",
            "   adam outputs dir: /Users/savvas/Library/CloudStorage/OneDrive-BTHStudent/4 THESIS/thesis-code/security_requirements_system/generations/adam/outputs\n",
            "   adam outputs exists: True\n",
            "   Files in test_outputs: 0 files\n",
            "\n",
            "âœ“ All available dependencies loaded from cache\n"
          ]
        }
      ],
      "source": [
        "# Helper: Load all cached outputs (run this to load dependencies)\n",
        "\n",
        "\n",
        "def load_all_dependencies():\n",
        "    \"\"\"Load all crew outputs from cache into memory.\"\"\"\n",
        "    global analysis_output, architecture_output, stakeholder_output\n",
        "    global threat_output, domain_output, ai_security_output, compliance_output\n",
        "    global security_architecture_output, roadmap_output, verification_output, validation_output\n",
        "    global components_json, threats_json, security_controls_json\n",
        "\n",
        "    from security_requirements_system.data_models import (\n",
        "        AnalysisOutput,\n",
        "        ArchitectureOutput,\n",
        "        ThreatModelingOutput,\n",
        "        DomainSecurityOutput,\n",
        "    )\n",
        "\n",
        "    # 1. Requirements Analysis\n",
        "    if cached := load_cached_output(\"requirements_analysis\"):\n",
        "        analysis_task = next((t for t in cached[\"tasks\"] if t[\"name\"] == \"analyze_requirements\"), None)\n",
        "        arch_task = next((t for t in cached[\"tasks\"] if t[\"name\"] == \"analyze_architecture\"), None)\n",
        "\n",
        "        if analysis_task and \"pydantic\" in analysis_task:\n",
        "            analysis_output = AnalysisOutput(**analysis_task[\"pydantic\"])\n",
        "        if arch_task and \"pydantic\" in arch_task:\n",
        "            architecture_output = ArchitectureOutput(**arch_task[\"pydantic\"])\n",
        "            components_json = json.dumps([c for c in arch_task[\"pydantic\"][\"components\"]])\n",
        "\n",
        "    # 2. Stakeholder\n",
        "    if cached := load_cached_output(\"stakeholder\"):\n",
        "        stakeholder_output = cached.get(\"raw\", \"\")\n",
        "\n",
        "    # 3. Threat Modeling\n",
        "    if cached := load_cached_output(\"threat_modeling\"):\n",
        "        if \"pydantic\" in cached:\n",
        "            threat_output = ThreatModelingOutput(**cached[\"pydantic\"])\n",
        "            threats_json = json.dumps(cached[\"pydantic\"])\n",
        "\n",
        "    # 4. Domain Security\n",
        "    if cached := load_cached_output(\"domain_security\"):\n",
        "        if \"tasks\" in cached and len(cached[\"tasks\"]) > 0:\n",
        "            if \"pydantic\" in cached[\"tasks\"][0]:\n",
        "                domain_output = DomainSecurityOutput(**cached[\"tasks\"][0][\"pydantic\"])\n",
        "                security_controls_json = json.dumps(cached[\"tasks\"][0][\"pydantic\"])\n",
        "\n",
        "    # 5. AI/LLM Security\n",
        "    if cached := load_cached_output(\"llm_security\"):\n",
        "        ai_security_output = cached.get(\"raw\", \"\")\n",
        "\n",
        "    # 6. Compliance\n",
        "    if cached := load_cached_output(\"compliance\"):\n",
        "        compliance_output = cached.get(\"raw\", \"\")\n",
        "\n",
        "    # 7. Security Architecture\n",
        "    if cached := load_cached_output(\"security_architecture\"):\n",
        "        security_architecture_output = cached.get(\"raw\", \"\")\n",
        "\n",
        "    # 8. Roadmap\n",
        "    if cached := load_cached_output(\"roadmap\"):\n",
        "        roadmap_output = cached.get(\"raw\", \"\")\n",
        "\n",
        "    # 9. Verification\n",
        "    if cached := load_cached_output(\"verification\"):\n",
        "        verification_output = cached.get(\"raw\", \"\")\n",
        "\n",
        "    # 10. Validation\n",
        "    if cached := load_cached_output(\"validation\"):\n",
        "        if \"tasks\" in cached and len(cached[\"tasks\"]) > 0:\n",
        "            if \"pydantic\" in cached[\"tasks\"][0]:\n",
        "                from security_requirements_system.data_models import ValidationOutput\n",
        "\n",
        "                validation_output = ValidationOutput(**cached[\"tasks\"][0][\"pydantic\"])\n",
        "\n",
        "    print(\"\\nâœ“ All available dependencies loaded from cache\")\n",
        "\n",
        "\n",
        "# Uncomment to auto-load all cached outputs:\n",
        "load_all_dependencies()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. Requirements Analysis Crew\n",
        "\n",
        "Analyzes product requirements and extracts security-relevant information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from security_requirements_system.crews.requirements_analysis_crew import RequirementsAnalysisCrew\n",
        "\n",
        "print(\"Running Requirements Analysis Crew...\\n\")\n",
        "\n",
        "crew = RequirementsAnalysisCrew().crew()\n",
        "result = crew.kickoff(inputs={\"requirements_text\": INPUT_REQUIREMENTS})\n",
        "\n",
        "display_crew_output(result, \"Requirements Analysis\")\n",
        "save_output(result, \"requirements_analysis\")\n",
        "\n",
        "# Store outputs for downstream crews\n",
        "analysis_task = next(filter(lambda x: x.name == \"analyze_requirements\", result.tasks_output))\n",
        "architecture_task = next(filter(lambda x: x.name == \"analyze_architecture\", result.tasks_output))\n",
        "\n",
        "analysis_output = analysis_task.pydantic\n",
        "architecture_output = architecture_task.pydantic\n",
        "\n",
        "print(f\"\\nâœ“ Application Summary: {analysis_output.application_summary[:150]}...\")\n",
        "print(f\"âœ“ High-level requirements: {len(analysis_output.high_level_requirements)}\")\n",
        "print(f\"âœ“ Architecture: {architecture_output.architecture_summary[:150]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Stakeholder Crew\n",
        "\n",
        "Identifies stakeholders and trust boundaries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from security_requirements_system.crews.stakeholder_crew import StakeholderCrew\n",
        "\n",
        "print(\"Running Stakeholder Crew...\\n\")\n",
        "\n",
        "# Load dependencies from cache if not in memory\n",
        "if \"architecture_output\" not in globals():\n",
        "    print(\"âš ï¸  Loading dependencies from cache...\")\n",
        "    from security_requirements_system.data_models import ArchitectureOutput\n",
        "\n",
        "    cached = load_cached_output(\"requirements_analysis\")\n",
        "    if cached and \"tasks\" in cached:\n",
        "        arch_task = next((t for t in cached[\"tasks\"] if t[\"name\"] == \"analyze_architecture\"), None)\n",
        "        if arch_task and \"pydantic\" in arch_task:\n",
        "            architecture_output = ArchitectureOutput(**arch_task[\"pydantic\"])\n",
        "\n",
        "crew = StakeholderCrew().crew()\n",
        "result = crew.kickoff(inputs={\"requirements_text\": INPUT_REQUIREMENTS, \"architecture_summary\": architecture_output.architecture_summary})\n",
        "\n",
        "display_crew_output(result, \"Stakeholder Analysis\")\n",
        "save_output(result, \"stakeholder\")\n",
        "\n",
        "stakeholder_output = result.raw\n",
        "print(f\"\\nâœ“ Output length: {len(stakeholder_output)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Threat Modeling Crew\n",
        "\n",
        "Performs STRIDE threat modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from security_requirements_system.crews.threat_modeling_crew import ThreatModelingCrew\n",
        "\n",
        "print(\"Running Threat Modeling Crew...\\n\")\n",
        "\n",
        "# Load dependencies from cache if not in memory\n",
        "if \"architecture_output\" not in globals():\n",
        "    print(\"âš ï¸  Loading dependencies from cache...\")\n",
        "    from security_requirements_system.data_models import ArchitectureOutput\n",
        "\n",
        "    cached = load_cached_output(\"requirements_analysis\")\n",
        "    if cached and \"tasks\" in cached:\n",
        "        arch_task = next((t for t in cached[\"tasks\"] if t[\"name\"] == \"analyze_architecture\"), None)\n",
        "        if arch_task and \"pydantic\" in arch_task:\n",
        "            architecture_output = ArchitectureOutput(**arch_task[\"pydantic\"])\n",
        "\n",
        "# Prepare components JSON\n",
        "components_json = json.dumps([c.model_dump() for c in architecture_output.components]) if architecture_output.components else \"[]\"\n",
        "\n",
        "crew = ThreatModelingCrew().crew()\n",
        "result = crew.kickoff(\n",
        "    inputs={\n",
        "        \"requirements_text\": INPUT_REQUIREMENTS,\n",
        "        \"architecture_summary\": architecture_output.architecture_summary,\n",
        "        \"components\": components_json,\n",
        "    }\n",
        ")\n",
        "\n",
        "display_crew_output(result, \"Threat Modeling\")\n",
        "save_output(result, \"threat_modeling\")\n",
        "\n",
        "threat_output = result.pydantic\n",
        "threats_json = json.dumps(threat_output.model_dump())\n",
        "print(f\"\\nâœ“ Threats identified: {len(threat_output.threats)}\")\n",
        "print(f\"\\nTop 5 threats:\")\n",
        "for i, threat in enumerate(threat_output.threats[:5], 1):\n",
        "    print(f\"{i}. {threat.threat_id}: {threat.description[:80]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Domain Security Crew (OWASP ASVS)\n",
        "\n",
        "Maps requirements to OWASP ASVS controls.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from security_requirements_system.crews.domain_security_crew import DomainSecurityCrew\n",
        "\n",
        "print(\"Running Domain Security Crew...\\n\")\n",
        "\n",
        "# Load dependencies from cache if not in memory\n",
        "if \"analysis_output\" not in globals():\n",
        "    print(\"âš ï¸  Loading dependencies from cache...\")\n",
        "    from security_requirements_system.data_models import AnalysisOutput\n",
        "\n",
        "    cached = load_cached_output(\"requirements_analysis\")\n",
        "    if cached and \"tasks\" in cached:\n",
        "        analysis_task = next((t for t in cached[\"tasks\"] if t[\"name\"] == \"analyze_requirements\"), None)\n",
        "        if analysis_task and \"pydantic\" in analysis_task:\n",
        "            analysis_output = AnalysisOutput(**analysis_task[\"pydantic\"])\n",
        "\n",
        "crew = DomainSecurityCrew().crew()\n",
        "result = crew.kickoff(inputs={\"high_level_requirements\": json.dumps(analysis_output.high_level_requirements)})\n",
        "\n",
        "display_crew_output(result, \"Domain Security\")\n",
        "save_output(result, \"domain_security\")\n",
        "\n",
        "domain_output = result.tasks_output[0].pydantic\n",
        "security_controls_json = json.dumps(domain_output.model_dump())\n",
        "\n",
        "# Count controls from all standards\n",
        "total_controls = 0\n",
        "controls_by_standard = {\"OWASP\": 0, \"NIST\": 0, \"ISO27001\": 0}\n",
        "\n",
        "for rm in domain_output.requirements_mapping:\n",
        "    all_controls = rm.security_controls\n",
        "    total_controls += len(all_controls)\n",
        "\n",
        "    # Count by standard\n",
        "    for ctrl in all_controls:\n",
        "        # Handle both Pydantic models and dicts\n",
        "        if isinstance(ctrl, dict):\n",
        "            standard = ctrl.get(\"standard\", \"OWASP\")\n",
        "        else:\n",
        "            standard = getattr(ctrl, \"standard\", \"OWASP\")\n",
        "\n",
        "        if standard in controls_by_standard:\n",
        "            controls_by_standard[standard] += 1\n",
        "        elif standard:\n",
        "            controls_by_standard[standard] = controls_by_standard.get(standard, 0) + 1\n",
        "\n",
        "print(f\"\\nâœ“ Requirements mapped: {len(domain_output.requirements_mapping)}\")\n",
        "print(f\"âœ“ Total controls (all standards): {total_controls}\")\n",
        "print(f\"\\n  Control breakdown by standard:\")\n",
        "for std, count in sorted(controls_by_standard.items()):\n",
        "    if count > 0:\n",
        "        print(f\"    - {std}: {count} controls\")\n",
        "if domain_output.recommended_asvs_level:\n",
        "    print(f\"\\nâœ“ Recommended ASVS level (OWASP): {domain_output.recommended_asvs_level}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. LLM Security Crew\n",
        "\n",
        "Identifies AI/ML security requirements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from security_requirements_system.crews.llm_security_crew import LLMSecurityCrew\n",
        "\n",
        "print(\"Running LLM Security Crew...\\n\")\n",
        "\n",
        "# Load dependencies from cache if not in memory\n",
        "if \"analysis_output\" not in globals():\n",
        "    print(\"âš ï¸  Loading dependencies from cache...\")\n",
        "    from security_requirements_system.data_models import AnalysisOutput\n",
        "\n",
        "    cached = load_cached_output(\"requirements_analysis\")\n",
        "    if cached and \"tasks\" in cached:\n",
        "        analysis_task = next((t for t in cached[\"tasks\"] if t[\"name\"] == \"analyze_requirements\"), None)\n",
        "        if analysis_task and \"pydantic\" in analysis_task:\n",
        "            analysis_output = AnalysisOutput(**analysis_task[\"pydantic\"])\n",
        "\n",
        "crew = LLMSecurityCrew().crew()\n",
        "result = crew.kickoff(\n",
        "    inputs={\n",
        "        \"requirements_text\": INPUT_REQUIREMENTS,\n",
        "        \"analyzed_requirements\": f\"Application Summary: {analysis_output.application_summary}\\nHigh-Level Requirements: {analysis_output.high_level_requirements}\",\n",
        "    }\n",
        ")\n",
        "\n",
        "display_crew_output(result, \"LLM Security\")\n",
        "save_output(result, \"llm_security\")\n",
        "\n",
        "ai_security_output = result.raw\n",
        "print(f\"\\nâœ“ Output length: {len(ai_security_output)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Compliance Crew\n",
        "\n",
        "Identifies compliance requirements (GDPR, PCI-DSS, etc.).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from security_requirements_system.crews.compliance_crew import ComplianceCrew\n",
        "\n",
        "print(\"Running Compliance Crew...\\n\")\n",
        "\n",
        "# Load dependencies from cache if not in memory\n",
        "if \"analysis_output\" not in globals():\n",
        "    print(\"âš ï¸  Loading dependencies from cache...\")\n",
        "    from security_requirements_system.data_models import AnalysisOutput\n",
        "\n",
        "    cached = load_cached_output(\"requirements_analysis\")\n",
        "    if cached and \"tasks\" in cached:\n",
        "        analysis_task = next((t for t in cached[\"tasks\"] if t[\"name\"] == \"analyze_requirements\"), None)\n",
        "        if analysis_task and \"pydantic\" in analysis_task:\n",
        "            analysis_output = AnalysisOutput(**analysis_task[\"pydantic\"])\n",
        "\n",
        "crew = ComplianceCrew().crew()\n",
        "result = crew.kickoff(\n",
        "    inputs={\n",
        "        \"requirements_text\": INPUT_REQUIREMENTS,\n",
        "        \"analyzed_requirements\": f\"Application Summary: {analysis_output.application_summary}\\nHigh-Level Requirements: {analysis_output.high_level_requirements}\",\n",
        "    }\n",
        ")\n",
        "\n",
        "display_crew_output(result, \"Compliance\")\n",
        "save_output(result, \"compliance\")\n",
        "\n",
        "compliance_output = result.raw\n",
        "print(f\"\\nâœ“ Output length: {len(compliance_output)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Security Architecture Crew\n",
        "\n",
        "Designs security architecture recommendations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from security_requirements_system.crews.security_architecture_crew import SecurityArchitectureCrew\n",
        "\n",
        "print(\"Running Security Architecture Crew...\\n\")\n",
        "\n",
        "# Load dependencies from cache if not in memory\n",
        "if \"architecture_output\" not in globals() or \"components_json\" not in globals() or \"security_controls_json\" not in globals():\n",
        "    print(\"âš ï¸  Loading dependencies from cache...\")\n",
        "    from security_requirements_system.data_models import ArchitectureOutput, DomainSecurityOutput\n",
        "\n",
        "    # Load architecture\n",
        "    if \"architecture_output\" not in globals():\n",
        "        cached = load_cached_output(\"requirements_analysis\")\n",
        "        if cached and \"tasks\" in cached:\n",
        "            arch_task = next((t for t in cached[\"tasks\"] if t[\"name\"] == \"analyze_architecture\"), None)\n",
        "            if arch_task and \"pydantic\" in arch_task:\n",
        "                architecture_output = ArchitectureOutput(**arch_task[\"pydantic\"])\n",
        "                components_json = json.dumps([c.model_dump() for c in architecture_output.components])\n",
        "\n",
        "    # Load security controls\n",
        "    if \"security_controls_json\" not in globals():\n",
        "        cached = load_cached_output(\"domain_security\")\n",
        "        if cached and \"tasks\" in cached and len(cached[\"tasks\"]) > 0:\n",
        "            if \"pydantic\" in cached[\"tasks\"][0]:\n",
        "                security_controls_json = json.dumps(cached[\"tasks\"][0][\"pydantic\"])\n",
        "\n",
        "crew = SecurityArchitectureCrew().crew()\n",
        "result = crew.kickoff(\n",
        "    inputs={\n",
        "        \"requirements_text\": INPUT_REQUIREMENTS,\n",
        "        \"architecture_summary\": architecture_output.architecture_summary,\n",
        "        \"components\": components_json,\n",
        "        \"security_controls\": security_controls_json,\n",
        "    }\n",
        ")\n",
        "\n",
        "display_crew_output(result, \"Security Architecture\")\n",
        "save_output(result, \"security_architecture\")\n",
        "\n",
        "security_architecture_output = result.raw\n",
        "print(f\"\\nâœ“ Output length: {len(security_architecture_output)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Implementation Roadmap Crew\n",
        "\n",
        "Creates phased implementation plan.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from security_requirements_system.crews.roadmap_crew import RoadmapCrew\n",
        "\n",
        "print(\"Running Roadmap Crew...\\n\")\n",
        "\n",
        "# Load dependencies from cache if not in memory\n",
        "if \"security_controls_json\" not in globals() or \"threats_json\" not in globals() or \"compliance_output\" not in globals():\n",
        "    print(\"âš ï¸  Loading dependencies from cache...\")\n",
        "\n",
        "    # Load security controls\n",
        "    if \"security_controls_json\" not in globals():\n",
        "        cached = load_cached_output(\"domain_security\")\n",
        "        if cached and \"tasks\" in cached and len(cached[\"tasks\"]) > 0:\n",
        "            if \"pydantic\" in cached[\"tasks\"][0]:\n",
        "                security_controls_json = json.dumps(cached[\"tasks\"][0][\"pydantic\"])\n",
        "\n",
        "    # Load threats\n",
        "    if \"threats_json\" not in globals():\n",
        "        cached = load_cached_output(\"threat_modeling\")\n",
        "        if cached and \"pydantic\" in cached:\n",
        "            threats_json = json.dumps(cached[\"pydantic\"])\n",
        "\n",
        "    # Load compliance\n",
        "    if \"compliance_output\" not in globals():\n",
        "        cached = load_cached_output(\"compliance\")\n",
        "        if cached:\n",
        "            compliance_output = cached.get(\"raw\", \"\")\n",
        "\n",
        "crew = RoadmapCrew().crew()\n",
        "result = crew.kickoff(\n",
        "    inputs={\n",
        "        \"requirements_text\": INPUT_REQUIREMENTS,\n",
        "        \"security_controls\": security_controls_json,\n",
        "        \"threats\": threats_json,\n",
        "        \"compliance_requirements\": compliance_output,\n",
        "    }\n",
        ")\n",
        "\n",
        "display_crew_output(result, \"Implementation Roadmap\")\n",
        "save_output(result, \"roadmap\")\n",
        "\n",
        "roadmap_output = result.raw\n",
        "print(f\"\\nâœ“ Output length: {len(roadmap_output)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Verification Crew\n",
        "\n",
        "Defines verification and testing strategy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from security_requirements_system.crews.verification_crew import VerificationCrew\n",
        "\n",
        "print(\"Running Verification Crew...\\n\")\n",
        "\n",
        "# Load dependencies from cache if not in memory\n",
        "if \"security_controls_json\" not in globals() or \"compliance_output\" not in globals():\n",
        "    print(\"âš ï¸  Loading dependencies from cache...\")\n",
        "\n",
        "    # Load security controls\n",
        "    if \"security_controls_json\" not in globals():\n",
        "        cached = load_cached_output(\"domain_security\")\n",
        "        if cached and \"tasks\" in cached and len(cached[\"tasks\"]) > 0:\n",
        "            if \"pydantic\" in cached[\"tasks\"][0]:\n",
        "                security_controls_json = json.dumps(cached[\"tasks\"][0][\"pydantic\"])\n",
        "\n",
        "    # Load compliance\n",
        "    if \"compliance_output\" not in globals():\n",
        "        cached = load_cached_output(\"compliance\")\n",
        "        if cached:\n",
        "            compliance_output = cached.get(\"raw\", \"\")\n",
        "\n",
        "crew = VerificationCrew().crew()\n",
        "result = crew.kickoff(\n",
        "    inputs={\n",
        "        \"security_controls\": security_controls_json,\n",
        "        \"compliance_requirements\": compliance_output,\n",
        "        \"owasp_controls\": security_controls_json,\n",
        "    }\n",
        ")\n",
        "\n",
        "display_crew_output(result, \"Verification & Testing\")\n",
        "save_output(result, \"verification\")\n",
        "\n",
        "verification_output = result.raw\n",
        "print(f\"\\nâœ“ Output length: {len(verification_output)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Validation Crew\n",
        "\n",
        "Validates completeness and quality of generated requirements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from security_requirements_system.crews.validation_crew import ValidationCrew\n",
        "\n",
        "print(\"Running Validation Crew...\\n\")\n",
        "\n",
        "# Load dependencies from cache if not in memory\n",
        "if \"analysis_output\" not in globals() or \"domain_output\" not in globals():\n",
        "    print(\"âš ï¸  Loading dependencies from cache...\")\n",
        "    from security_requirements_system.data_models import AnalysisOutput, DomainSecurityOutput\n",
        "\n",
        "    # Load requirements analysis\n",
        "    if \"analysis_output\" not in globals():\n",
        "        cached = load_cached_output(\"requirements_analysis\")\n",
        "        if cached and \"tasks\" in cached:\n",
        "            analysis_task = next((t for t in cached[\"tasks\"] if t[\"name\"] == \"analyze_requirements\"), None)\n",
        "            if analysis_task and \"pydantic\" in analysis_task:\n",
        "                analysis_output = AnalysisOutput(**analysis_task[\"pydantic\"])\n",
        "\n",
        "    # Load domain security\n",
        "    if \"domain_output\" not in globals():\n",
        "        cached = load_cached_output(\"domain_security\")\n",
        "        if cached and \"tasks\" in cached and len(cached[\"tasks\"]) > 0:\n",
        "            if \"pydantic\" in cached[\"tasks\"][0]:\n",
        "                domain_output = DomainSecurityOutput(**cached[\"tasks\"][0][\"pydantic\"])\n",
        "\n",
        "crew = ValidationCrew().crew()\n",
        "result = crew.kickoff(\n",
        "    inputs={\n",
        "        \"requirements_text\": INPUT_REQUIREMENTS,\n",
        "        \"analyzed_requirements\": f\"Application Summary: {analysis_output.application_summary}\\\\nHigh-Level Requirements: {analysis_output.high_level_requirements}\",\n",
        "        \"security_controls\": domain_output.model_dump_json(indent=2),\n",
        "        \"ai_security\": \"No AI components detected\",\n",
        "        \"compliance_requirements\": \"PCI-DSS, GDPR\",\n",
        "    }\n",
        ")\n",
        "\n",
        "display_crew_output(result, \"Validation\")\n",
        "save_output(result, \"validation\")\n",
        "\n",
        "validation_output = result.tasks_output[0].pydantic\n",
        "print(f\"\\nâœ“ Validation score: {validation_output.overall_score:.2f}\")\n",
        "print(f\"âœ“ Validation passed: {validation_output.validation_passed}\")\n",
        "print(f\"\\nDimension scores:\")\n",
        "for dim, score in validation_output.dimension_scores.items():\n",
        "    print(f\"  - {dim}: {score:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "All crew outputs saved to `test_outputs/` directory.\n",
        "\n",
        "Run any cell above to test specific crews. Outputs are cached to JSON files for inspection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Testing Main Flow Internal Methods\n",
        "\n",
        "Test the internal methods from `main.py` like `_export_dashboard_artifacts` and `_generate_markdown_summary` using cached crew outputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup mock flow state for testing internal methods\n",
        "from security_requirements_system.main import SecurityRequirementsFlow\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"Setting up mock flow state with cached data...\\n\")\n",
        "\n",
        "# Load all cached outputs\n",
        "load_all_dependencies()\n",
        "\n",
        "# Create a mock flow instance\n",
        "flow = SecurityRequirementsFlow()\n",
        "\n",
        "# Populate state with cached data\n",
        "if \"analysis_output\" in globals():\n",
        "    flow.state.application_summary = analysis_output.application_summary\n",
        "    flow.state.high_level_requirements = analysis_output.high_level_requirements\n",
        "\n",
        "    # Load detailed requirements if available\n",
        "    cached = load_cached_output(\"requirements_analysis\")\n",
        "    if cached and \"tasks\" in cached:\n",
        "        analysis_task = next((t for t in cached[\"tasks\"] if t[\"name\"] == \"analyze_requirements\"), None)\n",
        "        if analysis_task and \"pydantic\" in analysis_task:\n",
        "            detailed_reqs = analysis_task[\"pydantic\"].get(\"detailed_requirements\", [])\n",
        "            if detailed_reqs:\n",
        "                flow.state.detailed_requirements = json.dumps(\n",
        "                    [r.model_dump() if hasattr(r, \"model_dump\") else r for r in detailed_reqs], indent=2\n",
        "                )\n",
        "\n",
        "if \"architecture_output\" in globals():\n",
        "    flow.state.architecture_summary = architecture_output.architecture_summary\n",
        "    flow.state.architecture_diagram = architecture_output.architecture_diagram\n",
        "    if architecture_output.components:\n",
        "        flow.state.components = json.dumps(\n",
        "            [c.model_dump() if hasattr(c, \"model_dump\") else c for c in architecture_output.components], indent=2\n",
        "        )\n",
        "\n",
        "if \"stakeholder_output\" in globals():\n",
        "    flow.state.stakeholders = stakeholder_output\n",
        "\n",
        "if \"threat_output\" in globals():\n",
        "    flow.state.threats = json.dumps(threat_output.model_dump(), indent=2)\n",
        "\n",
        "if \"domain_output\" in globals():\n",
        "    flow.state.security_controls = json.dumps(domain_output.model_dump(), indent=2)\n",
        "\n",
        "if \"ai_security_output\" in globals():\n",
        "    flow.state.ai_security = ai_security_output\n",
        "\n",
        "if \"compliance_output\" in globals():\n",
        "    flow.state.compliance_requirements = compliance_output\n",
        "\n",
        "if \"security_architecture_output\" in globals():\n",
        "    flow.state.security_architecture = security_architecture_output\n",
        "\n",
        "if \"roadmap_output\" in globals():\n",
        "    flow.state.implementation_roadmap = roadmap_output\n",
        "\n",
        "if \"verification_output\" in globals():\n",
        "    flow.state.verification_testing = verification_output\n",
        "\n",
        "if \"validation_output\" in globals():\n",
        "    flow.state.validation_report = json.dumps(validation_output.model_dump(), indent=2)\n",
        "    flow.state.validation_score = validation_output.overall_score\n",
        "    flow.state.validation_passed = validation_output.validation_passed\n",
        "\n",
        "# Set sample requirements text\n",
        "flow.state.requirements_text = INPUT_REQUIREMENTS\n",
        "flow.state.iteration_count = 1\n",
        "\n",
        "# Build traceability matrix if not already set\n",
        "if not flow.state.traceability_matrix:\n",
        "    try:\n",
        "        flow.build_traceability_matrix()\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Could not build traceability matrix: {e}\")\n",
        "\n",
        "print(\"âœ“ Mock flow state created\")\n",
        "print(f\"  - Requirements: {len(flow.state.high_level_requirements)}\")\n",
        "print(f\"  - Has threats: {bool(flow.state.threats)}\")\n",
        "print(f\"  - Has controls: {bool(flow.state.security_controls)}\")\n",
        "print(f\"  - Has traceability matrix: {bool(flow.state.traceability_matrix)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test: _export_dashboard_artifacts\n",
        "\n",
        "Test the dashboard artifacts export function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test _export_dashboard_artifacts\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "artifacts_dir = OUTPUTS_DIR / \"artifacts\"\n",
        "artifacts_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Clean up old artifacts\n",
        "for file in artifacts_dir.glob(\"*.json\"):\n",
        "    file.unlink()\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "print(\"Testing _export_dashboard_artifacts...\\n\")\n",
        "try:\n",
        "    flow._export_dashboard_artifacts(artifacts_dir, timestamp)\n",
        "\n",
        "    print(\"âœ“ Dashboard artifacts exported successfully\\n\")\n",
        "\n",
        "    # List exported files\n",
        "    artifact_files = list(artifacts_dir.glob(\"*.json\"))\n",
        "    print(f\"Exported {len(artifact_files)} artifact files:\\n\")\n",
        "\n",
        "    for file in sorted(artifact_files):\n",
        "        size = file.stat().st_size / 1024  # KB\n",
        "        print(f\"  - {file.name} ({size:.2f} KB)\")\n",
        "\n",
        "        # Preview first few lines of each file\n",
        "        with open(file, \"r\") as f:\n",
        "            content = json.load(f)\n",
        "            if isinstance(content, list):\n",
        "                print(f\"    â†’ {len(content)} items\")\n",
        "                if len(content) > 0:\n",
        "                    print(f\"    â†’ Sample keys: {list(content[0].keys()) if isinstance(content[0], dict) else 'N/A'}\")\n",
        "            elif isinstance(content, dict):\n",
        "                print(f\"    â†’ Keys: {list(content.keys())}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error: {e}\")\n",
        "    import traceback\n",
        "\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect specific artifact files\n",
        "inspect_file = \"asvs_mapping.json\"  # Change this to inspect different files\n",
        "\n",
        "artifact_path = artifacts_dir / inspect_file\n",
        "if artifact_path.exists():\n",
        "    print(f\"Inspecting {inspect_file}:\\n\")\n",
        "    with open(artifact_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    if isinstance(data, list):\n",
        "        print(f\"Total items: {len(data)}\\n\")\n",
        "        if len(data) > 0:\n",
        "            print(\"Sample entries:\\n\")\n",
        "            for i, item in enumerate(data[:3], 1):\n",
        "                print(f\"{i}. {json.dumps(item, indent=2)}\\n\")\n",
        "            if len(data) > 3:\n",
        "                print(f\"... and {len(data) - 3} more entries\")\n",
        "    else:\n",
        "        print(json.dumps(data, indent=2))\n",
        "else:\n",
        "    print(f\"File {inspect_file} not found\")\n",
        "    print(f\"Available files: {[f.name for f in artifacts_dir.glob('*.json')]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test: _generate_markdown_summary\n",
        "\n",
        "Test the markdown generation function and inspect the output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Place output file in project root so relative path to test_outputs/test_artifacts works\n",
        "# This ensures the relative path calculation in _generate_markdown_summary works correctly\n",
        "test_output_path = PROJECT_ROOT / \"test_security_requirements.qmd\"\n",
        "\n",
        "print(\"Testing _generate_markdown_summary...\\n\")\n",
        "print(f\"Output will be saved to: {test_output_path}\\n\")\n",
        "\n",
        "try:\n",
        "    flow._generate_markdown_summary(test_output_path, artifacts_dir)\n",
        "\n",
        "    print(\"âœ“ Markdown summary generated successfully\\n\")\n",
        "\n",
        "    # Show file stats\n",
        "    if test_output_path.exists():\n",
        "        size = test_output_path.stat().st_size / 1024  # KB\n",
        "        with open(test_output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            lines = f.readlines()\n",
        "            line_count = len(lines)\n",
        "            char_count = sum(len(line) for line in lines)\n",
        "\n",
        "        print(f\"File statistics:\")\n",
        "        print(f\"  - Size: {size:.2f} KB\")\n",
        "        print(f\"  - Lines: {line_count:,}\")\n",
        "        print(f\"  - Characters: {char_count:,}\\n\")\n",
        "\n",
        "        # Show first 100 lines\n",
        "        print(\"First 100 lines preview:\\n\")\n",
        "        print(\"=\" * 80)\n",
        "        with open(test_output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for i, line in enumerate(f, 1):\n",
        "                if i <= 100:\n",
        "                    print(f\"{i:4d}| {line.rstrip()}\")\n",
        "                else:\n",
        "                    break\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"\\n... (file continues for {line_count - 100} more lines)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error: {e}\")\n",
        "    import traceback\n",
        "\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search for specific sections in the generated markdown\n",
        "if test_output_path.exists():\n",
        "    with open(test_output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # Find section headers\n",
        "    import re\n",
        "\n",
        "    sections = re.findall(r\"^##+\\s+(.+)$\", content, re.MULTILINE)\n",
        "\n",
        "    print(\"Found sections:\\n\")\n",
        "    for i, section in enumerate(sections[:30], 1):  # Show first 30\n",
        "        print(f\"{i:2d}. {section}\")\n",
        "\n",
        "    if len(sections) > 30:\n",
        "        print(f\"\\n... and {len(sections) - 30} more sections\")\n",
        "\n",
        "    # Search for specific content\n",
        "    search_terms = [\n",
        "        (\"Requirement IDs\", r\"#### 6\\.2\\.\\d+\\.\\s+([^\\n]+)\"),\n",
        "        (\"OWASP Controls\", r\"Control\\s+([A-Z]\\d+\\.\\d+\\.\\d+)\"),\n",
        "        (\"Threat IDs\", r\"Threat ID[^\\n]+\"),\n",
        "    ]\n",
        "\n",
        "    print(\"\\n\\nSearch results:\\n\")\n",
        "    for term, pattern in search_terms:\n",
        "        matches = re.findall(pattern, content)\n",
        "        print(f\"{term}: Found {len(matches)} matches\")\n",
        "        if matches:\n",
        "            print(f\"  Sample: {matches[:3]}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate requirement IDs in Section 6.2\n",
        "if test_output_path.exists():\n",
        "    with open(test_output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        content = f.read()\n",
        "\n",
        "    import re\n",
        "\n",
        "    # Find all section 6.2 subsections\n",
        "    pattern = r\"#### 6\\.2\\.(\\d+)\\.\\s+([^\\n]+):\\s+([^\\n]+)\"\n",
        "    matches = re.findall(pattern, content)\n",
        "\n",
        "    print(\"Section 6.2 Subsections:\\n\")\n",
        "    print(\"Index | Requirement ID | Requirement Text\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    none_count = 0\n",
        "    for idx, req_id, req_text in matches:\n",
        "        if req_id.lower() == \"none\" or not req_id.strip():\n",
        "            none_count += 1\n",
        "            print(f\"{idx:5s} | âŒ {req_id} | {req_text[:60]}...\")\n",
        "        else:\n",
        "            print(f\"{idx:5s} | âœ“ {req_id} | {req_text[:60]}...\")\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"\\nTotal subsections: {len(matches)}\")\n",
        "    print(f\"With valid IDs: {len(matches) - none_count}\")\n",
        "    print(f\"With 'None' or empty IDs: {none_count}\")\n",
        "\n",
        "    if none_count > 0:\n",
        "        print(f\"\\nâš ï¸ Warning: {none_count} subsections still have 'None' as requirement ID\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test: build_traceability_matrix\n",
        "\n",
        "Test the traceability matrix building function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test build_traceability_matrix\n",
        "print(\"Testing build_traceability_matrix...\\n\")\n",
        "\n",
        "# Clear existing matrix to test building\n",
        "flow.state.traceability_matrix = \"\"\n",
        "\n",
        "try:\n",
        "    flow.build_traceability_matrix()\n",
        "\n",
        "    if flow.state.traceability_matrix:\n",
        "        matrix_data = json.loads(flow.state.traceability_matrix)\n",
        "        entries = matrix_data.get(\"entries\", [])\n",
        "        summary = matrix_data.get(\"summary\", \"\")\n",
        "\n",
        "        print(\"âœ“ Traceability matrix built successfully\\n\")\n",
        "        print(f\"Summary: {summary}\\n\")\n",
        "        print(f\"Total entries: {len(entries)}\\n\")\n",
        "\n",
        "        if entries:\n",
        "            print(\"Sample entries:\\n\")\n",
        "            for i, entry in enumerate(entries[:3], 1):\n",
        "                print(f\"{i}. {entry.get('req_id', 'N/A')}: {entry.get('high_level_requirement', 'N/A')[:60]}...\")\n",
        "                print(f\"   Threats: {len(entry.get('threat_ids', []))}\")\n",
        "                print(f\"   Controls: {len(entry.get('owasp_control_ids', []))}\")\n",
        "                print(f\"   Priority: {entry.get('priority', 'N/A')}\\n\")\n",
        "\n",
        "            # Statistics\n",
        "            with_threats = sum(1 for e in entries if e.get(\"threat_ids\"))\n",
        "            with_controls = sum(1 for e in entries if e.get(\"owasp_control_ids\"))\n",
        "\n",
        "            print(f\"Statistics:\")\n",
        "            print(f\"  - Entries with threats: {with_threats} ({with_threats/len(entries)*100:.1f}%)\")\n",
        "            print(f\"  - Entries with controls: {with_controls} ({with_controls/len(entries)*100:.1f}%)\")\n",
        "    else:\n",
        "        print(\"âš ï¸ Traceability matrix is empty\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error: {e}\")\n",
        "    import traceback\n",
        "\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper: Compare Generated Outputs\n",
        "\n",
        "Compare different runs or validate output quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare requirement ID mapping in artifacts vs markdown\n",
        "print(\"Comparing requirement IDs across outputs:\\n\")\n",
        "\n",
        "# From detailed requirements\n",
        "if flow.state.detailed_requirements:\n",
        "    detailed_reqs = json.loads(flow.state.detailed_requirements)\n",
        "    req_ids_from_details = {}\n",
        "    for req in detailed_reqs:\n",
        "        req_text = req.get(\"requirement_text\", \"\").strip().lower()\n",
        "        req_id = req.get(\"requirement_id\", \"\")\n",
        "        if req_text and req_id:\n",
        "            req_ids_from_details[req_text] = req_id\n",
        "\n",
        "# From ASVS mapping artifacts\n",
        "asvs_file = test_artifacts_dir / \"asvs_mapping.json\"\n",
        "req_ids_from_asvs = set()\n",
        "if asvs_file.exists():\n",
        "    with open(asvs_file, \"r\") as f:\n",
        "        asvs_data = json.load(f)\n",
        "        for item in asvs_data:\n",
        "            req_id = item.get(\"requirement_id\")\n",
        "            if req_id:\n",
        "                req_ids_from_asvs.add(req_id)\n",
        "\n",
        "# From markdown\n",
        "if test_output_path.exists():\n",
        "    with open(test_output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        md_content = f.read()\n",
        "\n",
        "    import re\n",
        "\n",
        "    md_section_pattern = r\"#### 6\\.2\\.\\d+\\.\\s+([A-Z]+-\\d+):\"\n",
        "    req_ids_from_md = set(re.findall(md_section_pattern, md_content))\n",
        "\n",
        "print(\"Requirement IDs found:\\n\")\n",
        "print(f\"From detailed requirements: {len(req_ids_from_details)}\")\n",
        "print(f\"  Sample: {list(req_ids_from_details.values())[:5]}\\n\")\n",
        "\n",
        "print(f\"From ASVS mapping artifacts: {len(req_ids_from_asvs)}\")\n",
        "print(f\"  Sample: {list(req_ids_from_asvs)[:5]}\\n\")\n",
        "\n",
        "print(f\"From markdown section 6.2: {len(req_ids_from_md)}\")\n",
        "print(f\"  Sample: {list(req_ids_from_md)[:5]}\\n\")\n",
        "\n",
        "# Check for consistency\n",
        "if req_ids_from_details and req_ids_from_asvs:\n",
        "    overlap = req_ids_from_asvs.intersection(set(req_ids_from_details.values()))\n",
        "    print(f\"Overlap between detailed reqs and ASVS: {len(overlap)}/{len(req_ids_from_asvs)}\")\n",
        "\n",
        "if req_ids_from_asvs and req_ids_from_md:\n",
        "    overlap_md = req_ids_from_asvs.intersection(req_ids_from_md)\n",
        "    print(f\"Overlap between ASVS and markdown: {len(overlap_md)}/{len(req_ids_from_md)}\")\n",
        "\n",
        "    missing_in_md = req_ids_from_asvs - req_ids_from_md\n",
        "    if missing_in_md:\n",
        "        print(f\"\\nâš ï¸ IDs in ASVS but not in markdown: {missing_in_md}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all saved outputs\n",
        "import os\n",
        "\n",
        "output_dir = Path(\"test_outputs\")\n",
        "if output_dir.exists():\n",
        "    print(\"\\nSaved outputs:\")\n",
        "    for file in sorted(output_dir.glob(\"*.json\")):\n",
        "        size = file.stat().st_size / 1024  # KB\n",
        "        print(f\"  - {file.name} ({size:.1f} KB)\")\n",
        "else:\n",
        "    print(\"\\nNo outputs saved yet. Run cells above to generate outputs.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
